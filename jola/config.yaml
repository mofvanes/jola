model_config:
  pretrained_model_name_or_path: "meta-llama/Llama-3.2-1B-Instruct"
  device: "cuda"
  cache_dir: "./tmp/"
  applied_module: 'attention'
  base_model_name: 'llama3.2_1B_chat'

training_config:
  learning_rate: 0.005
  lr_scheduler_type: 'cosine'
  warmup_steps: 50
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  num_train_epochs: 20
  eval_strategy: 'epoch'
  save_strategy: 'epoch'
  load_best_model_at_end: True
  save_total_limit: 1
  logging_strategy: "epoch"
  seed: 42
  do_train: True
  do_eval: True
  bf16: False
  output_dir: './output'

data_config:
  train_size: 200
  task_name: "common_reason"
  data_path: "./dataset"

jola_config:
  gate_lambda: 0.00004
  gate_scheduler: "expon"